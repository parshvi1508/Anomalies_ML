# ğŸ—ï¸ COMPLETE SYSTEM ARCHITECTURE & WORKING LOGIC

## ğŸ“Š SYSTEM OVERVIEW

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         USER INTERFACE (Next.js)                     â”‚
â”‚  Dashboard | Students | Recommendations | Upload | Analysis | Models â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    API LAYER (Next.js API Routes)                    â”‚
â”‚  /api/students  |  /api/recommendations  |  /api/analyze-python    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Python Child Process  â”‚ (spawn Python scripts from Node.js)
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     PYTHON PROCESSING LAYER                          â”‚
â”‚  1. Recommender System   2. Anomaly Detection   3. Data Analysis    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DATA LAYER (CSV Files)                            â”‚
â”‚  courses.csv | user_preferences.csv | interactions.csv | models/    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ COMPLETE DATA FLOW

### **Flow 1: Recommendation System**

```
1. USER ACTION (Frontend)
   â”œâ”€ User enters User ID: "U001"
   â”œâ”€ Selects top_n: 5
   â””â”€ Clicks "Get Recommendations"

2. API REQUEST (Next.js API Route)
   â”œâ”€ GET /api/recommendations?user_id=U001&top_n=5
   â””â”€ Route handler: src/app/api/recommendations/route.ts

3. PYTHON SCRIPT EXECUTION
   â”œâ”€ spawn('python', ['scripts/get_recommendations.py', '{"user_id":"U001","top_n":5}'])
   â””â”€ Script receives parameters as JSON string

4. DATA LOADING (Python)
   â”œâ”€ Load data/courses.csv â†’ DataFrame with 25 courses
   â”œâ”€ Load data/user_preferences.csv â†’ User profile for U001
   â””â”€ Load data/user_course_interactions.csv â†’ U001's interaction history

5. HYBRID RECOMMENDER INITIALIZATION
   â”œâ”€ ContentBasedRecommender
   â”‚   â”œâ”€ Create TF-IDF matrix from course descriptions
   â”‚   â”œâ”€ Shape: (25 courses Ã— 100 features)
   â”‚   â””â”€ Compute cosine similarity matrix (25Ã—25)
   â”‚
   â”œâ”€ CollaborativeFilteringRecommender
   â”‚   â”œâ”€ Load interaction ratings
   â”‚   â”œâ”€ Train SVD model (50 factors, 20 epochs)
   â”‚   â””â”€ Split: 80% train, 20% test
   â”‚
   â””â”€ HybridRecommender combines both

6. RECOMMENDATION GENERATION
   â”œâ”€ Step 1: Content-Based Score
   â”‚   â”œâ”€ Build user profile from U001's completed courses
   â”‚   â”œâ”€ Weight by ratings: course_vector * implicit_rating
   â”‚   â”œâ”€ Compute similarity: cosine(user_profile, all_courses)
   â”‚   â””â”€ Output: content_score for each course (0-1)
   â”‚
   â”œâ”€ Step 2: Collaborative Filtering Score
   â”‚   â”œâ”€ For each candidate course:
   â”‚   â”‚   â””â”€ predicted_rating = SVD.predict(U001, course_id)
   â”‚   â””â”€ Normalize to 0-1: cf_score = predicted_rating / 5.0
   â”‚
   â”œâ”€ Step 3: Rule-Based Score
   â”‚   â”œâ”€ Match domain interests: +0.3 if domain matches
   â”‚   â”œâ”€ Match learning pace: +0.2 if format suits pace
   â”‚   â”œâ”€ Match cost preference: +0.25 if cost aligns
   â”‚   â”œâ”€ Match platform preference: +0.15 if platform preferred
   â”‚   â”œâ”€ Match difficulty: +0.1 if difficulty matches level
   â”‚   â””â”€ Normalize: rule_score (0-1)
   â”‚
   â”œâ”€ Step 4: Popularity Score
   â”‚   â”œâ”€ Count enrollments per course
   â”‚   â”œâ”€ Combine with rating: (rating/5)*0.6 + (enrollments/10)*0.4
   â”‚   â””â”€ Output: popularity_score (0-1)
   â”‚
   â””â”€ Step 5: Hybrid Combination
       â”œâ”€ hybrid_score = content_score*0.35 + cf_score*0.40 + 
       â”‚                 rule_score*0.15 + popularity_score*0.10
       â”œâ”€ Sort by hybrid_score descending
       â””â”€ Return top 5 courses

7. JSON RESPONSE
   {
     "user_id": "U001",
     "recommendations": [
       {
         "course_id": "C002",
         "title": "Advanced Machine Learning",
         "hybrid_score": 0.8523,
         "content_score": 0.7841,
         "cf_score": 0.9012,
         ...
       }
     ],
     "count": 5
   }

8. FRONTEND DISPLAY
   â””â”€ Render recommendation cards with scores
```

---

### **Flow 2: Student Risk Assessment**

```
1. DATA SOURCE
   uploads/student_data_with_risk.csv
   â”œâ”€ Contains: student_id, gpa, attendance, risk_score, risk_category
   â””â”€ Generated by final_anomaly&dropout.ipynb notebook

2. API REQUEST
   GET /api/students

3. DATA PROCESSING (route.ts)
   â”œâ”€ Read CSV file with csv-parser
   â”œâ”€ Parse each row:
   â”‚   â”œâ”€ student_id: string
   â”‚   â”œâ”€ risk_score: parseFloat()
   â”‚   â””â”€ risk_category: 'Low Risk' | 'Moderate Risk' | 'High Risk' | 'Extreme Risk'
   â””â”€ Group by risk_category

4. RESPONSE
   {
     "Low Risk": [{ student_id, name, risk_score, ... }],
     "Moderate Risk": [...],
     "High Risk": [...],
     "Extreme Risk": [...]
   }

5. FRONTEND
   â”œâ”€ /students page displays all students
   â”œâ”€ Filter by risk category
   â”œâ”€ Sort by risk score
   â””â”€ Click student â†’ /students/[id] detail page
```

---

### **Flow 3: Anomaly Detection & Dropout Prediction**

**This happens in the Jupyter Notebook (offline training):**

```
STEP 1: DATA GENERATION
â”œâ”€ Generate synthetic student data (10,000 students)
â”œâ”€ Features: gpa, attendance, failed_courses, feedback_engagement,
â”‚            late_assignments, forum_participation, clicks_per_week, etc.
â””â”€ Save: student_dropout_dataset.csv

STEP 2: ANOMALY DETECTION (Isolation Forest)
â”œâ”€ Input Features: clicks_per_week, days_active, forum_participation
â”œâ”€ Model: IsolationForest(contamination=0.1, n_estimators=100)
â”œâ”€ Training:
â”‚   â”œâ”€ Build isolation trees by random splitting
â”‚   â”œâ”€ Anomalies have shorter path lengths (easier to isolate)
â”‚   â””â”€ Compute anomaly_score for each student (0-1)
â”œâ”€ Output: 
â”‚   â”œâ”€ anomaly_score: 0 (normal) to 1 (anomaly)
â”‚   â””â”€ is_anomaly: -1 (anomaly) or 1 (normal)
â””â”€ Save: public/models/anomaly_model.pkl

STEP 3: FEATURE ENHANCEMENT
â”œâ”€ Add anomaly_score to original dataset
â”œâ”€ Create interaction features:
â”‚   â”œâ”€ anomaly_gpa = anomaly_score * gpa
â”‚   â”œâ”€ anomaly_attendance = anomaly_score * attendance
â”‚   â””â”€ anomaly_engagement = anomaly_score * feedback_engagement
â””â”€ Enhanced feature set: 16 original + 4 anomaly features

STEP 4: DROPOUT PREDICTION (Random Forest)
â”œâ”€ Input: All features (20 total)
â”œâ”€ Model: RandomForestClassifier(n_estimators=100, class_weight='balanced')
â”œâ”€ SMOTE: Balance classes (oversample minority dropout class)
â”œâ”€ Training:
â”‚   â”œâ”€ 80% train, 20% test
â”‚   â”œâ”€ Build 100 decision trees
â”‚   â”œâ”€ Each tree votes on dropout/non-dropout
â”‚   â””â”€ Majority vote = final prediction
â”œâ”€ Output:
â”‚   â”œâ”€ dropout_probability (0-1)
â”‚   â””â”€ dropout_prediction (0 or 1)
â””â”€ Save: public/models/dropout_model.pkl

STEP 5: DEMPSTER-SHAFER EVIDENCE COMBINATION
â”œâ”€ Combine 3 evidence sources:
â”‚   1. Anomaly Score
â”‚   2. Dropout Probability
â”‚   3. Expert Rules (low GPA, poor attendance, failed courses)
â”‚
â”œâ”€ Convert to Mass Functions:
â”‚   m(dropout) = probability * (1 - uncertainty)
â”‚   m(no_dropout) = (1 - probability) * (1 - uncertainty)
â”‚   m(Î˜) = uncertainty (ignorance)
â”‚
â”œâ”€ Dempster's Rule of Combination:
â”‚   mâ‚â‚‚(A) = Î£{mâ‚(X) Ã— mâ‚‚(Y) : Xâˆ©Y=A} / (1 - conflict)
â”‚
â”œâ”€ Calculate:
â”‚   â”œâ”€ Belief(dropout): Lower bound of certainty
â”‚   â”œâ”€ Plausibility(dropout): Upper bound of certainty
â”‚   â””â”€ Uncertainty: Plausibility - Belief
â”‚
â””â”€ Output:
    â”œâ”€ belief_score (0-1)
    â”œâ”€ plausibility_score (0-1)
    â”œâ”€ uncertainty (0-1)
    â””â”€ risk_category: Based on belief threshold

STEP 6: RISK CATEGORIZATION
â”œâ”€ If belief_score >= 0.7 â†’ "Extreme Risk"
â”œâ”€ If belief_score >= 0.5 â†’ "High Risk"
â”œâ”€ If belief_score >= 0.3 â†’ "Moderate Risk"
â””â”€ If belief_score < 0.3 â†’ "Low Risk"

STEP 7: SAVE RESULTS
â”œâ”€ Save models:
â”‚   â”œâ”€ public/models/anomaly_model.pkl
â”‚   â”œâ”€ public/models/dropout_model.pkl
â”‚   â””â”€ public/models/ds_combiner.pkl
â””â”€ Save predictions: uploads/student_data_with_risk.csv
```

---

### **Flow 4: Data Upload & Analysis**

```
1. USER UPLOADS CSV
   â”œâ”€ /upload page
   â”œâ”€ Select student_data.csv
   â””â”€ Click upload

2. API REQUEST
   POST /api/analyze-python
   â”œâ”€ FormData with file
   â””â”€ File validation: .csv only

3. FILE PROCESSING
   â”œâ”€ Save to temp/[timestamp]_[filename].csv
   â””â”€ Pass file path to Python script

4. PYTHON ANALYSIS
   scripts/explore_student_data.py
   â”œâ”€ Load CSV into pandas DataFrame
   â”œâ”€ Calculate statistics:
   â”‚   â”œâ”€ Total records
   â”‚   â”œâ”€ Total features
   â”‚   â””â”€ Dropout rate
   â”œâ”€ Generate visualizations:
   â”‚   â”œâ”€ Correlation heatmap (matplotlib/seaborn)
   â”‚   â”œâ”€ Feature distributions by dropout
   â”‚   â”œâ”€ Boxplots by dropout status
   â”‚   â””â”€ GPA vs Attendance scatter plot
   â”œâ”€ Convert plots to base64 images
   â””â”€ Return JSON with plots

5. RESPONSE
   {
     "overview": { total_records, total_features, dropout_rate },
     "plots": {
       "correlation_heatmap": "data:image/png;base64,...",
       "feature_distributions": "data:image/png;base64,...",
       ...
     }
   }

6. FRONTEND DISPLAY
   â””â”€ Render base64 images in <Image> components
```

---

## ğŸ§® MODEL DETAILS

### **Model 1: Content-Based Recommender**

**Input:**
```python
courses_df: DataFrame
    - course_id, title, description, domain, difficulty, 
      format, platform, learning_objectives, rating

user_interactions: DataFrame
    - user_id, course_id, rating, time_spent_hours, 
      completion_status, implicit_rating
```

**Processing:**
```python
# Step 1: TF-IDF Vectorization
combined_text = title + domain + description + objectives
tfidf_matrix = TfidfVectorizer(max_features=100).fit_transform(combined_text)
# Shape: (25 courses, 100 features)

# Step 2: User Profile
user_courses = interactions[user_id's completed courses]
user_vector = weighted_average(
    course_vectors, 
    weights=implicit_ratings
)
# Shape: (1, 100)

# Step 3: Similarity
similarities = cosine_similarity(user_vector, tfidf_matrix)
# Shape: (1, 25)

# Step 4: Rank
top_courses = courses.sort_by(similarities, descending=True).head(5)
```

**Output:**
```python
DataFrame: [course_id, title, similarity_score (0-1)]
```

---

### **Model 2: Collaborative Filtering (SVD)**

**Input:**
```python
ratings_matrix: DataFrame
    user_id | course_id | rating (1-5)
    U001    | C001      | 5
    U001    | C003      | 4
    U002    | C005      | 5
```

**Processing:**
```python
# Step 1: Matrix Factorization
# R â‰ˆ U Ã— V^T
# R: user-item rating matrix (mÃ—n)
# U: user factors (mÃ—k), k=50 latent factors
# V: item factors (nÃ—k)

svd = SVD(n_factors=50, n_epochs=20)
svd.fit(trainset)

# Step 2: Prediction
predicted_rating = U[user_id] Â· V[course_id]^T
# Dot product of user and item latent vectors

# Step 3: Normalize
cf_score = predicted_rating / 5.0  # Convert to 0-1
```

**Output:**
```python
predicted_rating: float (1-5 scale)
```

---

### **Model 3: Isolation Forest (Anomaly Detection)**

**Input:**
```python
behavioral_features: DataFrame
    clicks_per_week, days_active, forum_participation,
    study_group, meeting_attendance
```

**Processing:**
```python
# Build isolation trees
for each tree:
    1. Randomly sample features
    2. Randomly split data between min and max
    3. Recursively split until all points isolated
    
# Anomaly scoring
anomaly_score = 2^(-average_path_length / c(n))
# where c(n) = expected path length for n samples

# Shorter path = more anomalous
```

**Output:**
```python
anomaly_score: float (0-1)
is_anomaly: int (-1 or 1)
```

---

### **Model 4: Random Forest (Dropout Prediction)**

**Input:**
```python
student_features: DataFrame (20 features)
    gpa, attendance, failed_courses, late_assignments,
    anomaly_score, anomaly_gpa, anomaly_attendance, ...
```

**Processing:**
```python
# Build 100 decision trees
for each tree:
    1. Bootstrap sample from training data
    2. At each split, randomly select subset of features
    3. Find best split using Gini impurity
    4. Build tree until stopping criteria
    
# Prediction
dropout_probability = average(
    tree1.predict_proba(student),
    tree2.predict_proba(student),
    ...
    tree100.predict_proba(student)
)
```

**Output:**
```python
dropout_probability: float (0-1)
dropout_prediction: int (0 or 1)
```

---

### **Model 5: Dempster-Shafer Combiner**

**Input:**
```python
anomaly_score: float (0-1)
dropout_probability: float (0-1)
expert_rules: dict
    {
        'low_gpa': bool,
        'poor_attendance': bool,
        'failed_courses': bool
    }
```

**Processing:**
```python
# Convert to mass functions
def to_mass(prob, uncertainty):
    return {
        'dropout': prob * (1 - uncertainty),
        'no_dropout': (1 - prob) * (1 - uncertainty),
        'ignorance': uncertainty
    }

m1 = to_mass(anomaly_score, u1)
m2 = to_mass(dropout_probability, u2)
m3 = to_mass(expert_rules_score, u3)

# Dempster's combination
def combine(m1, m2):
    combined = {}
    for A in m1:
        for B in m2:
            intersection = A âˆ© B
            if intersection â‰  âˆ…:
                combined[intersection] += m1[A] * m2[B]
    
    # Normalize by conflict
    conflict = Î£(m1[A] * m2[B]) where A âˆ© B = âˆ…
    for key in combined:
        combined[key] /= (1 - conflict)
    
    return combined

final_mass = combine(combine(m1, m2), m3)

# Calculate belief and plausibility
belief = final_mass['dropout']
plausibility = belief + final_mass['ignorance']
uncertainty = plausibility - belief
```

**Output:**
```python
belief_score: float (0-1)
plausibility_score: float (0-1)
uncertainty: float (0-1)
risk_category: str
```

---

## ğŸš€ DEPLOYMENT TO NEXT.JS (PRODUCTION)

### **âœ… What WILL Work:**

1. **Next.js Frontend**
   - All React components âœ…
   - API Routes âœ…
   - Static assets âœ…
   - CSS/Tailwind âœ…

2. **CSV Data Storage**
   - CSV files in `/data` folder âœ…
   - CSV files in `/uploads` folder âœ…
   - Works on Vercel, Netlify, etc. âœ…

3. **Python Script Execution**
   - **IF** Python runtime is available âš ï¸
   - Works on: Railway, Render, AWS, DigitalOcean âœ…
   - **DOES NOT** work on: Vercel (serverless) âŒ

### **âš ï¸ DEPLOYMENT CHALLENGES:**

#### **Challenge 1: Python Runtime**

**Problem:** Vercel (most common Next.js host) doesn't support Python child processes

**Solutions:**

**Option A: Use Vercel Python Runtime**
```typescript
// Convert to serverless functions
// api/recommendations.py (Python serverless function)
// api/recommendations/route.ts calls api/recommendations.py
```

**Option B: Separate Python Backend**
```
Frontend (Next.js) â†’ Vercel
Backend (Python FastAPI) â†’ Railway/Render/Heroku
   â†“
API Gateway pattern
```

**Option C: Use Platform with Python Support**
- Railway âœ… (Node + Python)
- Render âœ… (Node + Python)
- AWS Elastic Beanstalk âœ…
- DigitalOcean App Platform âœ…

#### **Challenge 2: File System Access**

**Problem:** Serverless environments have read-only file systems (except /tmp)

**Solutions:**
- Store CSV files in object storage (AWS S3, Vercel Blob)
- Use database instead (PostgreSQL, MongoDB)
- Embed small datasets in code

#### **Challenge 3: Model Loading**

**Problem:** Loading large .pkl files on cold starts is slow

**Solutions:**
- Pre-load models in serverless function
- Use model serving platforms (AWS SageMaker, Google AI Platform)
- Cache models in memory

---

## ğŸ“‹ RECOMMENDED DEPLOYMENT ARCHITECTURE

### **Architecture 1: Monolithic (Railway/Render)**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Railway/Render Container         â”‚
â”‚                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Next.js App (Node.js)      â”‚  â”‚
â”‚  â”‚  - Frontend                  â”‚  â”‚
â”‚  â”‚  - API Routes                â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚               â”‚                    â”‚
â”‚               â–¼                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Python Scripts              â”‚  â”‚
â”‚  â”‚  - Recommender System        â”‚  â”‚
â”‚  â”‚  - spawn() calls work        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚               â”‚                    â”‚
â”‚               â–¼                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  CSV Files & Models          â”‚  â”‚
â”‚  â”‚  - /data/*.csv               â”‚  â”‚
â”‚  â”‚  - /public/models/*.pkl      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Pros:** Simple, everything in one place
**Cons:** Harder to scale independently

---

### **Architecture 2: Microservices (Recommended for Scale)**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Vercel (Frontend) â”‚
â”‚  Next.js App       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”‚ HTTPS
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  API Gateway / Load Balancer   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
     â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚           â”‚              â”‚
     â–¼           â–¼              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Recommendâ”‚ â”‚ Anomaly â”‚  â”‚ Analysis â”‚
â”‚ Service â”‚ â”‚ Service â”‚  â”‚ Service  â”‚
â”‚(FastAPI)â”‚ â”‚(FastAPI)â”‚  â”‚(FastAPI) â”‚
â”‚ Python  â”‚ â”‚ Python  â”‚  â”‚ Python   â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚           â”‚              â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚  Cloud Storage       â”‚
      â”‚  - CSV Files (S3)    â”‚
      â”‚  - Models (S3)       â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Pros:** Scalable, independent deployment
**Cons:** More complex, higher cost

---

## ğŸ› ï¸ DEPLOYMENT STEPS

### **Option 1: Railway (Easiest)**

```bash
# 1. Install Railway CLI
npm install -g @railway/cli

# 2. Login
railway login

# 3. Initialize project
railway init

# 4. Add start script to package.json
{
  "scripts": {
    "dev": "next dev",
    "build": "next build",
    "start": "next start",
    "railway:build": "npm install && pip install -r requirements.txt && npm run build"
  }
}

# 5. Create railway.toml
[build]
builder = "NIXPACKS"
buildCommand = "npm run railway:build"

[deploy]
startCommand = "npm start"
restartPolicyType = "ON_FAILURE"
restartPolicyMaxRetries = 10

# 6. Deploy
railway up
```

### **Option 2: Render**

```yaml
# render.yaml (already in your project!)
services:
  - type: web
    name: dropout-dashboard
    env: node
    buildCommand: npm install && pip install -r requirements.txt && npm run build
    startCommand: npm start
    envVars:
      - key: NODE_ENV
        value: production
```

### **Option 3: Docker (Any Platform)**

```dockerfile
# Dockerfile
FROM node:18-alpine

# Install Python
RUN apk add --no-cache python3 py3-pip

WORKDIR /app

# Copy package files
COPY package*.json ./
COPY requirements.txt ./

# Install dependencies
RUN npm install
RUN pip3 install -r requirements.txt

# Copy app
COPY . .

# Build Next.js
RUN npm run build

EXPOSE 3000

CMD ["npm", "start"]
```

---

## âœ… FINAL DEPLOYMENT CHECKLIST

- [ ] Python runtime available on hosting platform
- [ ] All dependencies in requirements.txt
- [ ] CSV files committed to repo or uploaded to storage
- [ ] Trained models (.pkl files) in public/models/
- [ ] Environment variables configured
- [ ] Build command includes Python dependencies
- [ ] File paths are relative, not absolute
- [ ] Error handling for missing files
- [ ] CORS configured if using separate backend
- [ ] API routes tested in production

---

## ğŸ¯ SUMMARY

**Your system works as:**

1. **Next.js frontend** displays UI
2. **API Routes** receive requests
3. **Python scripts** execute via spawn()
4. **CSV files** provide data storage
5. **Trained models** (.pkl) provide predictions
6. **JSON responses** return to frontend

**For production:**
- Use Railway/Render for easiest deployment âœ…
- Python + Node.js in same container works perfectly âœ…
- CSV files work fine for current scale âœ…
- Consider microservices for enterprise scale âš¡

Your codebase is **deployment-ready** for platforms that support both Node.js and Python!
