# üìä TECHNICAL METHODOLOGY & MODEL DOCUMENTATION

## Table of Contents
1. [Anomaly Detection System](#1-anomaly-detection-system)
2. [Dropout Prediction System](#2-dropout-prediction-system)
3. [Dempster-Shafer Evidence Combination](#3-dempster-shafer-evidence-combination)
4. [Recommendation System](#4-recommendation-system)
5. [Data Flow & Integration](#5-data-flow--integration)

---

## 1. ANOMALY DETECTION SYSTEM

### 1.1 Algorithm: Isolation Forest

**Purpose:** Detect anomalous student behavior patterns based on engagement metrics.

### 1.2 Input Data

```
Features Used:
- clicks_per_week: Number of clicks student makes per week
- days_active: Number of days student is active
- forum_participation: Level of forum engagement (0-10)
- study_group: Binary indicator (0 or 1)
- meeting_attendance: Percentage of meetings attended (0-100)
```

**Data Format:**
```python
X = df[['clicks_per_week', 'days_active', 'forum_participation', 
        'study_group', 'meeting_attendance']]
# Shape: (n_students, 5 features)
```

### 1.3 Mathematical Model

**Isolation Forest Algorithm:**

The algorithm isolates observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature.

**Anomaly Score Calculation:**

```
h(x) = path length of sample x
c(n) = average path length of unsuccessful search in BST
      = 2H(n-1) - (2(n-1)/n)
      where H(i) = ln(i) + Euler's constant (‚âà 0.5772)

Anomaly Score(x) = 2^(-h(x)/c(n))
```

**Interpretation:**
- Score ‚âà 1: Clear anomaly
- Score ‚âà 0.5: Normal sample
- Score ‚âà 0: Very normal sample

### 1.4 Model Parameters

```python
IsolationForest(
    contamination=0.1,      # Expected proportion of outliers (10%)
    n_estimators=100,       # Number of isolation trees
    max_samples='auto',     # Samples to draw: min(256, n_samples)
    random_state=42         # For reproducibility
)
```

### 1.5 Processing Steps

1. **Build Isolation Trees:**
   ```
   For each tree (100 trees):
       - Sample data points randomly
       - Select random feature
       - Select random split value between min and max
       - Recursively split until:
         * Node has only 1 sample, OR
         * All samples have same value, OR
         * Tree reaches max depth
   ```

2. **Calculate Path Length:**
   ```
   For each student:
       path_length = average traversal depth across all trees
   ```

3. **Compute Anomaly Score:**
   ```
   anomaly_score = 2^(-path_length / c(n))
   where n = number of samples
   ```

### 1.6 Output

```python
Output Format:
{
    'anomaly_score': float (0 to 1),
    'is_anomaly': int (-1 for anomaly, 1 for normal)
}

Example:
Student A: anomaly_score = 0.73 ‚Üí Anomalous behavior
Student B: anomaly_score = 0.42 ‚Üí Normal behavior
```

**Classification Rule:**
```
IF anomaly_score > contamination_threshold (0.1):
    is_anomaly = -1  (Anomaly)
ELSE:
    is_anomaly = 1   (Normal)
```

---

## 2. DROPOUT PREDICTION SYSTEM

### 2.1 Algorithm: Random Forest Classifier

**Purpose:** Predict whether a student will drop out based on academic and behavioral features.

### 2.2 Input Data

```
Original Features (16):
- gpa: Grade Point Average (0.0-4.0)
- attendance: Attendance percentage (0-100)
- failed_courses: Number of courses failed
- feedback_engagement: Feedback engagement level (0-10)
- late_assignments: Number of late submissions
- forum_participation: Forum activity level (0-10)
- clicks_per_week: Weekly platform clicks
- days_active: Active days per week
- study_group: Group study participation (0/1)
- meeting_attendance: Meeting attendance (0-100)
- resource_usage: Educational resource usage (0-10)
- course_completion: Completed courses count
- quiz_performance: Average quiz score (0-100)
- assignment_score: Average assignment score (0-100)
- midterm_score: Midterm exam score (0-100)
- final_score: Final exam score (0-100)

Enhanced Features (4):
- anomaly_gpa: anomaly_score √ó gpa
- anomaly_attendance: anomaly_score √ó attendance
- anomaly_engagement: anomaly_score √ó feedback_engagement
- anomaly_interaction: anomaly_score √ó (clicks_per_week/100)

Total Features: 20
```

**Data Format:**
```python
X = df[all_20_features]
y = df['dropout']  # Binary: 1 = dropout, 0 = continue
```

### 2.3 Mathematical Model

**Random Forest Algorithm:**

Ensemble of Decision Trees using Bagging (Bootstrap Aggregating)

**Gini Impurity (for splitting):**

```
Gini(D) = 1 - Œ£(p·µ¢¬≤)
where:
    D = dataset at node
    p·µ¢ = proportion of class i
    i ‚àà {dropout, continue}

For binary classification:
Gini(D) = 1 - (p_dropout¬≤ + p_continue¬≤)
```

**Information Gain:**

```
IG(D, feature) = Gini(D_parent) - Œ£(|D_child|/|D_parent| √ó Gini(D_child))
```

**Prediction (Classification):**

```
For each tree t in forest (100 trees):
    prediction_t = class with majority votes at leaf node

Final Prediction = Mode(prediction_1, prediction_2, ..., prediction_100)
```

**Dropout Probability:**

```
P(dropout) = (Number of trees predicting dropout) / (Total trees)
```

### 2.4 Model Parameters

```python
RandomForestClassifier(
    n_estimators=100,           # Number of trees
    max_depth=None,             # Unlimited depth
    min_samples_split=2,        # Min samples to split node
    min_samples_leaf=1,         # Min samples at leaf
    class_weight='balanced',    # Handle class imbalance
    random_state=42             # Reproducibility
)
```

**Class Weight Balancing:**
```
w_dropout = n_samples / (n_classes √ó n_dropout_samples)
w_continue = n_samples / (n_classes √ó n_continue_samples)
```

### 2.5 Processing Steps

1. **Handle Class Imbalance (SMOTE):**
   ```
   SMOTE (Synthetic Minority Over-sampling Technique):
   
   For each minority class sample x:
       1. Find k nearest neighbors (k=5)
       2. Randomly select one neighbor x_neighbor
       3. Generate synthetic sample:
          x_synthetic = x + Œª √ó (x_neighbor - x)
          where Œª ~ Uniform(0, 1)
   ```

2. **Build Random Forest:**
   ```
   For each tree (100 trees):
       1. Bootstrap sample: Draw n samples with replacement
       2. Build decision tree:
          At each node:
              - Randomly select ‚àö20 ‚âà 4 features
              - Find best split using Gini impurity
              - Split if IG > 0
              - Continue until stopping criteria
   ```

3. **Predict Dropout Probability:**
   ```
   For new student:
       For each tree:
           Traverse tree to leaf node
           Record class prediction
       
       dropout_probability = count(dropout_predictions) / 100
       dropout_prediction = 1 if dropout_probability > 0.5 else 0
   ```

### 2.6 Output

```python
Output Format:
{
    'dropout_probability': float (0 to 1),
    'dropout_prediction': int (0 or 1),
    'feature_importance': dict
}

Example:
Student X:
    dropout_probability = 0.73
    dropout_prediction = 1
    interpretation = "73% of trees predict dropout"
```

---

## 3. DEMPSTER-SHAFER EVIDENCE COMBINATION

### 3.1 Purpose

Combine multiple evidence sources (anomaly detection, dropout prediction, expert rules) to make a final risk assessment with uncertainty quantification.

### 3.2 Theory of Evidence

**Frame of Discernment:**
```
Œò = {dropout, no_dropout}
```

**Mass Function (Basic Probability Assignment):**
```
m: 2^Œò ‚Üí [0, 1]
where:
    Œ£ m(A) = 1 for all A ‚äÜ Œò
    m(‚àÖ) = 0
```

### 3.3 Input Data

**Three Evidence Sources:**

1. **Anomaly Score Evidence:**
   ```python
   anomaly_score = output from Isolation Forest (0-1)
   ```

2. **Dropout Probability Evidence:**
   ```python
   dropout_probability = output from Random Forest (0-1)
   ```

3. **Expert Rules Evidence:**
   ```python
   Rules:
   - low_gpa: gpa < 2.5
   - poor_attendance: attendance < 60%
   - failed_courses: failed_courses > 2
   - low_engagement: feedback_engagement < 3
   ```

### 3.4 Mass Function Conversion

**Formula for each evidence source:**

```
Given: probability p and uncertainty u

m(dropout) = p √ó (1 - u)
m(no_dropout) = (1 - p) √ó (1 - u)
m(Œò) = u  (ignorance/uncertainty)

where:
    p = probability of dropout
    u = uncertainty level (0-1)
```

**Example Conversion:**

```python
# Anomaly Evidence
anomaly_score = 0.73
uncertainty_anomaly = 0.2

m1({dropout}) = 0.73 √ó (1 - 0.2) = 0.584
m1({no_dropout}) = 0.27 √ó (1 - 0.2) = 0.216
m1(Œò) = 0.2

# Dropout Model Evidence
dropout_prob = 0.68
uncertainty_model = 0.15

m2({dropout}) = 0.68 √ó (1 - 0.15) = 0.578
m2({no_dropout}) = 0.32 √ó (1 - 0.15) = 0.272
m2(Œò) = 0.15

# Expert Rules Evidence
rules_score = 0.6  (60% of rules triggered)
uncertainty_rules = 0.3

m3({dropout}) = 0.6 √ó (1 - 0.3) = 0.42
m3({no_dropout}) = 0.4 √ó (1 - 0.3) = 0.28
m3(Œò) = 0.3
```

### 3.5 Dempster's Rule of Combination

**Formula:**

```
m‚ÇÅ‚ÇÇ(A) = [Œ£ m‚ÇÅ(B) √ó m‚ÇÇ(C)] / (1 - K)
         B‚à©C=A

Conflict Factor:
K = Œ£ m‚ÇÅ(B) √ó m‚ÇÇ(C)
    B‚à©C=‚àÖ
```

**Step-by-Step Combination:**

**Step 1: Combine m1 and m2**

```
Intersection Table:
                m2({D})  m2({ND})  m2(Œò)
m1({D})         {D}      ‚àÖ         {D}
m1({ND})        ‚àÖ        {ND}      {ND}
m1(Œò)           {D}      {ND}      Œò

where D = dropout, ND = no_dropout

Calculate combined masses:
m‚ÇÅ‚ÇÇ({dropout}) = m1({D})√óm2({D}) + m1({D})√óm2(Œò) + m1(Œò)√óm2({D})
                = 0.584√ó0.578 + 0.584√ó0.15 + 0.2√ó0.578
                = 0.337 + 0.088 + 0.116 = 0.541

m‚ÇÅ‚ÇÇ({no_dropout}) = m1({ND})√óm2({ND}) + m1({ND})√óm2(Œò) + m1(Œò)√óm2({ND})
                   = 0.216√ó0.272 + 0.216√ó0.15 + 0.2√ó0.272
                   = 0.059 + 0.032 + 0.054 = 0.145

m‚ÇÅ‚ÇÇ(Œò) = m1(Œò)√óm2(Œò)
        = 0.2√ó0.15 = 0.03

Conflict:
K = m1({D})√óm2({ND}) + m1({ND})√óm2({D})
  = 0.584√ó0.272 + 0.216√ó0.578
  = 0.159 + 0.125 = 0.284

Normalization (divide by 1-K):
m‚ÇÅ‚ÇÇ({dropout}) = 0.541 / (1-0.284) = 0.755
m‚ÇÅ‚ÇÇ({no_dropout}) = 0.145 / 0.716 = 0.203
m‚ÇÅ‚ÇÇ(Œò) = 0.03 / 0.716 = 0.042
```

**Step 2: Combine m‚ÇÅ‚ÇÇ with m3**

```
Follow same process to get final masses:
m_final({dropout})
m_final({no_dropout})
m_final(Œò)
```

### 3.6 Belief and Plausibility Calculation

**Belief (Lower Bound):**
```
Bel(A) = Œ£ m(B)
         B‚äÜA

Bel({dropout}) = m({dropout})
               = measure of definite support for dropout
```

**Plausibility (Upper Bound):**
```
Pl(A) = Œ£ m(B)
        B‚à©A‚â†‚àÖ

Pl({dropout}) = m({dropout}) + m(Œò)
              = maximum possible support for dropout
```

**Uncertainty:**
```
Uncertainty = Pl({dropout}) - Bel({dropout})
            = m(Œò)
            = degree of ignorance
```

### 3.7 Final Risk Categorization

**Rules:**

```python
belief_score = Bel({dropout})

IF belief_score >= 0.7:
    risk_category = "Extreme Risk"
    risk_level = 4
    
ELIF belief_score >= 0.5:
    risk_category = "High Risk"
    risk_level = 3
    
ELIF belief_score >= 0.3:
    risk_category = "Moderate Risk"
    risk_level = 2
    
ELSE:
    risk_category = "Low Risk"
    risk_level = 1
```

### 3.8 Output

```python
Final Output:
{
    'belief_score': float (0-1),
    'plausibility_score': float (0-1),
    'uncertainty': float (0-1),
    'risk_category': str,
    'risk_level': int (1-4),
    'contributing_factors': list
}

Example:
Student Y:
    belief_score = 0.755
    plausibility_score = 0.797
    uncertainty = 0.042
    risk_category = "Extreme Risk"
    interpretation = "At least 75.5% confidence in dropout risk"
```

---

## 4. RECOMMENDATION SYSTEM

### 4.1 Content-Based Filtering

**Purpose:** Recommend courses based on similarity to courses the user has taken.

#### 4.1.1 Input Data

```python
Courses DataFrame:
- course_id: Unique identifier
- title: Course name
- description: Course description
- domain: Subject area
- learning_objectives: Learning goals
- difficulty: Beginner/Intermediate/Advanced
- duration_weeks: Course length
- platform: Learning platform
- rating: Average rating (0-5)

User Interactions:
- user_id: Student identifier
- course_id: Course taken
- rating: User rating (1-5)
- time_spent_hours: Hours spent
- completion_status: Completed/In Progress
- implicit_rating: Derived score (0-1)
```

#### 4.1.2 TF-IDF Vectorization

**Term Frequency (TF):**
```
TF(term, document) = (Number of times term appears in document) / 
                     (Total terms in document)
```

**Inverse Document Frequency (IDF):**
```
IDF(term) = log(Total documents / Documents containing term)
```

**TF-IDF Score:**
```
TF-IDF(term, document) = TF(term, document) √ó IDF(term)
```

**Feature Construction:**
```python
# Combine text features
combined_text = title + " " + domain + " " + description + " " + learning_objectives

# Create TF-IDF matrix
tfidf_vectorizer = TfidfVectorizer(max_features=100)
tfidf_matrix = tfidf_vectorizer.fit_transform(combined_text)

# Shape: (25 courses, 100 features)
```

#### 4.1.3 Cosine Similarity

**Formula:**
```
Cosine Similarity(A, B) = (A ¬∑ B) / (||A|| √ó ||B||)

where:
    A ¬∑ B = Œ£(A·µ¢ √ó B·µ¢)  (dot product)
    ||A|| = ‚àö(Œ£(A·µ¢¬≤))  (magnitude)
```

**Computation:**
```python
# User profile vector (weighted average of completed courses)
user_vector = Œ£(course_vector_i √ó implicit_rating_i) / Œ£(implicit_rating_i)

# Similarity with all courses
for each course:
    similarity_score = cosine_similarity(user_vector, course_vector)
```

#### 4.1.4 Output

```python
Content-Based Score:
{
    'course_id': str,
    'content_score': float (0-1),
    'similarity_explanation': str
}

Interpretation:
score = 0.85 ‚Üí 85% similar to user's preferences
```

### 4.2 Collaborative Filtering

**Purpose:** Recommend courses based on similar users' preferences.

#### 4.2.1 Matrix Factorization (SVD)

**User-Item Rating Matrix:**
```
R = [r_ui]  (m users √ó n items)

where r_ui = rating of user u for item i
```

**SVD Decomposition:**
```
R ‚âà U √ó Œ£ √ó V^T

where:
    U: m √ó k user feature matrix
    Œ£: k √ó k diagonal matrix of singular values
    V: n √ó k item feature matrix
    k: number of latent factors (50)
```

**Predicted Rating:**
```
rÃÇ_ui = Œº + b_u + b_i + q_i^T √ó p_u

where:
    Œº = global average rating
    b_u = user bias (user's tendency to rate high/low)
    b_i = item bias (item's tendency to be rated high/low)
    p_u = user latent factor vector
    q_i = item latent factor vector
```

**Optimization (Gradient Descent):**
```
Minimize: Œ£(r_ui - rÃÇ_ui)¬≤ + Œª(||p_u||¬≤ + ||q_i||¬≤ + b_u¬≤ + b_i¬≤)

where Œª = regularization parameter (0.02)
```

#### 4.2.2 Model Parameters

```python
SVD(
    n_factors=50,          # Latent dimensions
    n_epochs=20,           # Training iterations
    lr_all=0.005,          # Learning rate
    reg_all=0.02,          # Regularization
    biased=True            # Use bias terms
)
```

#### 4.2.3 Training Process

```
For each epoch (20 iterations):
    For each rating r_ui in training set:
        1. Compute prediction: rÃÇ_ui = Œº + b_u + b_i + q_i^T √ó p_u
        2. Calculate error: e_ui = r_ui - rÃÇ_ui
        3. Update parameters:
           p_u ‚Üê p_u + Œ±(e_ui √ó q_i - Œª √ó p_u)
           q_i ‚Üê q_i + Œ±(e_ui √ó p_u - Œª √ó q_i)
           b_u ‚Üê b_u + Œ±(e_ui - Œª √ó b_u)
           b_i ‚Üê b_i + Œ±(e_ui - Œª √ó b_i)
        
        where Œ± = learning rate (0.005)
```

#### 4.2.4 Evaluation Metrics

**Root Mean Square Error (RMSE):**
```
RMSE = ‚àö[Œ£(r_ui - rÃÇ_ui)¬≤ / n]
```

**Mean Absolute Error (MAE):**
```
MAE = Œ£|r_ui - rÃÇ_ui| / n
```

#### 4.2.5 Output

```python
Collaborative Filtering Score:
{
    'course_id': str,
    'predicted_rating': float (1-5),
    'cf_score': float (0-1)  # normalized: predicted_rating/5
}
```

### 4.3 Rule-Based Scoring

**Purpose:** Apply domain-specific rules to boost relevant courses.

#### 4.3.1 Rules

```python
Rule 1: Domain Match
IF course.domain IN user.preferred_domains:
    score += 0.3

Rule 2: Learning Pace Match
IF course.format MATCHES user.learning_pace:
    score += 0.2
    # self-paced ‚Üî self_paced, instructor-led ‚Üî structured

Rule 3: Cost Preference Match
IF course.cost_type MATCHES user.cost_preference:
    score += 0.25
    # free ‚Üî free_only, paid ‚Üî willing_to_pay

Rule 4: Platform Preference
IF course.platform IN user.preferred_platforms:
    score += 0.15

Rule 5: Difficulty Match
IF course.difficulty MATCHES user.current_level:
    score += 0.1

Total Rule Score = Œ£(matched_rules) / 1.0  # Normalized
```

#### 4.3.2 Output

```python
Rule-Based Score:
{
    'course_id': str,
    'rule_score': float (0-1),
    'matched_rules': list
}
```

### 4.4 Popularity-Based Scoring

**Formula:**
```
Popularity Score = (rating/5) √ó 0.6 + (enrollments/max_enrollments) √ó 0.4

Normalized to [0, 1]
```

### 4.5 Hybrid Recommendation

**Purpose:** Combine all recommendation approaches.

#### 4.5.1 Weighted Combination

**Default Weights:**
```python
weights = {
    'content_based': 0.35,      # 35%
    'collaborative': 0.40,      # 40%
    'rule_based': 0.15,         # 15%
    'popularity': 0.10          # 10%
}
```

**Hybrid Score Calculation:**
```
hybrid_score = (content_score √ó 0.35) + 
               (cf_score √ó 0.40) + 
               (rule_score √ó 0.15) + 
               (popularity_score √ó 0.10)
```

#### 4.5.2 Cold Start Handling

**For New Users (no interaction history):**
```python
# Adjust weights to favor non-personalized approaches
weights = {
    'content_based': 0.20,      # Reduced (no history)
    'collaborative': 0.10,      # Reduced (no history)
    'rule_based': 0.40,         # Increased (use preferences)
    'popularity': 0.30          # Increased (general trends)
}
```

#### 4.5.3 At-Risk Student Recommendations

**Special Adjustments:**
```python
# Boost beginner and foundational courses
IF student.risk_category IN ['High Risk', 'Extreme Risk']:
    IF course.difficulty == 'Beginner':
        hybrid_score √ó 1.2
    IF 'foundational' in course.tags:
        hybrid_score √ó 1.15
    IF course.duration_weeks <= 6:
        hybrid_score √ó 1.1  # Prefer shorter courses
```

#### 4.5.4 Final Ranking

```
1. Calculate hybrid_score for all candidate courses
2. Sort by hybrid_score (descending)
3. Apply diversity filter (max 2 courses per domain)
4. Return top N recommendations
```

#### 4.5.5 Output

```python
Final Recommendation:
{
    'user_id': str,
    'recommendations': [
        {
            'course_id': str,
            'title': str,
            'hybrid_score': float (0-1),
            'content_score': float (0-1),
            'cf_score': float (0-1),
            'rule_score': float (0-1),
            'popularity_score': float (0-1),
            'explanation': str
        },
        ...
    ],
    'count': int
}
```

---

## 5. DATA FLOW & INTEGRATION

### 5.1 End-to-End Pipeline

```
INPUT DATA
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 1. FEATURE ENGINEERING                      ‚îÇ
‚îÇ    - Clean data                             ‚îÇ
‚îÇ    - Handle missing values                  ‚îÇ
‚îÇ    - Create derived features                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 2. ANOMALY DETECTION                        ‚îÇ
‚îÇ    Algorithm: Isolation Forest              ‚îÇ
‚îÇ    Input: 5 behavioral features             ‚îÇ
‚îÇ    Output: anomaly_score (0-1)              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 3. FEATURE ENHANCEMENT                      ‚îÇ
‚îÇ    - Create interaction features:           ‚îÇ
‚îÇ      * anomaly_score √ó gpa                  ‚îÇ
‚îÇ      * anomaly_score √ó attendance           ‚îÇ
‚îÇ      * etc.                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 4. DROPOUT PREDICTION                       ‚îÇ
‚îÇ    Algorithm: Random Forest                 ‚îÇ
‚îÇ    Input: 20 features (16 original + 4 new) ‚îÇ
‚îÇ    Output: dropout_probability (0-1)        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 5. EXPERT RULES EVALUATION                  ‚îÇ
‚îÇ    - Check: low GPA, poor attendance, etc.  ‚îÇ
‚îÇ    Output: rules_score (0-1)                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 6. DEMPSTER-SHAFER COMBINATION              ‚îÇ
‚îÇ    Inputs:                                  ‚îÇ
‚îÇ    - anomaly_score                          ‚îÇ
‚îÇ    - dropout_probability                    ‚îÇ
‚îÇ    - rules_score                            ‚îÇ
‚îÇ    Process:                                 ‚îÇ
‚îÇ    - Convert to mass functions              ‚îÇ
‚îÇ    - Apply Dempster's rule                  ‚îÇ
‚îÇ    Output:                                  ‚îÇ
‚îÇ    - belief_score                           ‚îÇ
‚îÇ    - plausibility_score                     ‚îÇ
‚îÇ    - uncertainty                            ‚îÇ
‚îÇ    - risk_category                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 7. COURSE RECOMMENDATIONS                   ‚îÇ
‚îÇ    (For at-risk students)                   ‚îÇ
‚îÇ    A. Content-Based: TF-IDF + Cosine Sim    ‚îÇ
‚îÇ    B. Collaborative: SVD Matrix Fact.       ‚îÇ
‚îÇ    C. Rule-Based: Domain rules              ‚îÇ
‚îÇ    D. Popularity: Rating √ó Enrollments      ‚îÇ
‚îÇ    E. Hybrid: Weighted combination          ‚îÇ
‚îÇ    Output: Top N recommended courses        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
FINAL OUTPUT
    - Student risk assessment
    - Personalized course recommendations
    - Intervention priorities
```

### 5.2 System Integration Points

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Frontend      ‚îÇ
‚îÇ  (Next.js)      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ HTTP REST API
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   API Routes    ‚îÇ
‚îÇ  (TypeScript)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ spawn()
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Python Scripts ‚îÇ
‚îÇ  - Anomaly Det  ‚îÇ
‚îÇ  - Dropout Pred ‚îÇ
‚îÇ  - DS Combiner  ‚îÇ
‚îÇ  - Recommender  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Data Layer    ‚îÇ
‚îÇ   (CSV Files)   ‚îÇ
‚îÇ  - students.csv ‚îÇ
‚îÇ  - courses.csv  ‚îÇ
‚îÇ  - interact.csv ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 5.3 Data Formats

**Student Risk Data:**
```json
{
  "student_id": "S0001",
  "features": {
    "gpa": 2.3,
    "attendance": 55,
    "clicks_per_week": 45
  },
  "predictions": {
    "anomaly_score": 0.73,
    "dropout_probability": 0.68,
    "belief_score": 0.755,
    "risk_category": "Extreme Risk"
  },
  "timestamp": "2025-12-30T10:00:00Z"
}
```

**Recommendation Data:**
```json
{
  "user_id": "U001",
  "request": {
    "top_n": 5,
    "filters": ["Data Science", "Beginner"]
  },
  "response": {
    "recommendations": [
      {
        "course_id": "C002",
        "title": "Advanced Machine Learning",
        "hybrid_score": 0.8523,
        "components": {
          "content": 0.7841,
          "collaborative": 0.9012,
          "rules": 0.8500,
          "popularity": 0.8800
        }
      }
    ]
  }
}
```

---

## 6. PERFORMANCE METRICS

### 6.1 Anomaly Detection Metrics

```
Contamination Rate: 10% (expected outliers)
Precision: Fraction of detected anomalies that are true anomalies
Recall: Fraction of true anomalies that are detected
F1-Score: Harmonic mean of Precision and Recall
```

### 6.2 Dropout Prediction Metrics

```
Accuracy: (TP + TN) / (TP + TN + FP + FN)
Precision: TP / (TP + FP)
Recall (Sensitivity): TP / (TP + FN)
Specificity: TN / (TN + FP)
F1-Score: 2 √ó (Precision √ó Recall) / (Precision + Recall)
AUC-ROC: Area under ROC curve

Current Performance:
- Accuracy: 72%
- Precision: 66.7%
- Specificity: 98.6%
```

### 6.3 Recommendation Metrics

```
RMSE: ‚àö[Œ£(actual - predicted)¬≤ / n]
MAE: Œ£|actual - predicted| / n
Precision@K: Relevant items in top K / K
Recall@K: Relevant items in top K / Total relevant
NDCG: Normalized Discounted Cumulative Gain

Current Performance:
- SVD RMSE: ~0.85
- SVD MAE: ~0.65
```

---

## 7. IMPLEMENTATION NOTES

### 7.1 Computational Complexity

```
Isolation Forest:
- Training: O(n √ó t √ó œà √ó log œà)
  where n = features, t = trees, œà = sample size
- Prediction: O(t √ó log œà)

Random Forest:
- Training: O(n √ó m √ó t √ó log n)
  where n = samples, m = features, t = trees
- Prediction: O(t √ó log n)

Dempster-Shafer:
- Combination: O(2^|Œò|)
  where |Œò| = 2 (binary frame)

Collaborative Filtering (SVD):
- Training: O(n √ó k √ó i)
  where n = ratings, k = factors, i = iterations
- Prediction: O(k)
```

### 7.2 Scalability Considerations

```
Current Scale:
- Students: 10,000
- Courses: 25
- User Interactions: 25
- Features: 20

Production Scale Recommendations:
- Use batch processing for >100K students
- Implement caching for recommendations
- Use approximate nearest neighbors for similarity
- Implement incremental model updates
```

---

## 8. REFERENCES

### 8.1 Algorithms

1. **Isolation Forest:** Liu, F. T., Ting, K. M., & Zhou, Z. H. (2008). Isolation forest. ICDM.
2. **Random Forest:** Breiman, L. (2001). Random forests. Machine learning.
3. **Dempster-Shafer Theory:** Shafer, G. (1976). A mathematical theory of evidence.
4. **SVD:** Koren, Y., Bell, R., & Volinsky, C. (2009). Matrix factorization techniques.

### 8.2 Libraries Used

```
Python:
- scikit-learn: ML algorithms
- scikit-surprise: Collaborative filtering
- pandas: Data manipulation
- numpy: Numerical computations
- scipy: Statistical functions

TypeScript/JavaScript:
- Next.js: Frontend framework
- React: UI components
- Chart.js: Visualizations
```

---

## SUMMARY

This system implements a comprehensive student success prediction and intervention system:

1. **Detects** anomalous behavior patterns (Isolation Forest)
2. **Predicts** dropout risk (Random Forest with SMOTE)
3. **Combines** multiple evidence sources (Dempster-Shafer Theory)
4. **Recommends** personalized interventions (Hybrid Recommender)
5. **Quantifies** uncertainty in predictions
6. **Provides** explainable results for educational decision-making

All models work together in a pipeline that processes student data, identifies at-risk students, and provides actionable recommendations for intervention.
