{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ML Report: Dropout Prediction and Evidence Fusion\n",
        "\n",
        "This notebook reproduces key figures and tables for the dissertation:\n",
        "- RF: ROC/PR with F1-optimal threshold, confusion matrix, metrics table\n",
        "- DS: Belief–plausibility scatter, risk-tier histogram vs true labels, interval coverage, specificity–recall vs threshold\n",
        "- Feature importance (RF)\n",
        "- Ablation: RF w/ vs w/o anomaly features\n",
        "- Decision analysis: cost-sensitive sweep / net benefit\n",
        "\n",
        "Outputs will be saved under `reports/figures/` and `reports/tables/`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'imblearn'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     11\u001b[39m     accuracy_score, precision_score, recall_score, f1_score,\n\u001b[32m     12\u001b[39m     confusion_matrix, roc_curve, auc, precision_recall_curve, roc_auc_score\n\u001b[32m     13\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mimblearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Paths\u001b[39;00m\n\u001b[32m     18\u001b[39m FIG_DIR = \u001b[33m'\u001b[39m\u001b[33mreports/figures\u001b[39m\u001b[33m'\u001b[39m\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'imblearn'"
          ]
        }
      ],
      "source": [
        "# Setup\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, roc_curve, auc, precision_recall_curve, roc_auc_score\n",
        ")\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Paths\n",
        "FIG_DIR = 'reports/figures'\n",
        "TAB_DIR = 'reports/tables'\n",
        "os.makedirs(FIG_DIR, exist_ok=True)\n",
        "os.makedirs(TAB_DIR, exist_ok=True)\n",
        "\n",
        "np.random.seed(42)\n",
        "sns.set_context('talk')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data: load synthetic from existing CSV if present, else generate\n",
        "CSV_PATH = 'uploads/student_data.csv'\n",
        "\n",
        "# Optional: prefer dataset with label `dropout`\n",
        "if os.path.exists(CSV_PATH):\n",
        "    df_raw = pd.read_csv(CSV_PATH)\n",
        "else:\n",
        "    # Fallback: generate synthetic consistent with the original notebook\n",
        "    def generate_student_data(n_samples=1000, dropout_rate=0.3):\n",
        "        student_ids = [f'S{i:04d}' for i in range(1, n_samples + 1)]\n",
        "        gpa = np.clip(np.random.normal(3.0, 0.7, n_samples), 0, 4)\n",
        "        attendance = np.clip(np.random.normal(85, 10, n_samples), 50, 100)\n",
        "        semester = np.random.randint(1, 9, n_samples)\n",
        "        prev_gpa = np.clip(gpa + np.random.normal(0, 0.3, n_samples), 0, 4)\n",
        "        failed_courses = np.random.poisson(1, n_samples)\n",
        "        feedback_engagement = np.clip(np.random.normal(70, 20, n_samples), 0, 100)\n",
        "        late_assignments = np.clip(np.random.normal(20, 15, n_samples), 0, 100)\n",
        "        forum_participation = np.random.poisson(3, n_samples)\n",
        "        meeting_attendance = np.clip(np.random.normal(80, 15, n_samples), 0, 100)\n",
        "        study_group = np.random.choice([0, 1, 2, 3], n_samples, p=[0.4, 0.3, 0.2, 0.1])\n",
        "        days_active = np.clip(np.random.normal(5, 1, n_samples), 0, 7)\n",
        "        clicks_per_week = np.clip(np.random.negative_binomial(10, 0.5, n_samples), 0, 500)\n",
        "        assessments_submitted = np.random.poisson(5, n_samples)\n",
        "        previous_attempts = np.random.poisson(0.7, n_samples)\n",
        "        studied_credits = np.random.randint(10, 40, n_samples)\n",
        "        risk_score = (\n",
        "            (gpa < 2.5).astype(float) * 0.4 +\n",
        "            (attendance < 70).astype(float) * 0.3 +\n",
        "            (failed_courses > 2).astype(float) * 0.2 +\n",
        "            (feedback_engagement < 50).astype(float) * 0.1\n",
        "        )\n",
        "        risk_score += np.random.normal(0, 0.1, n_samples)\n",
        "        dropout = (risk_score > 0.5).astype(int)\n",
        "        current_rate = dropout.mean()\n",
        "        if current_rate != dropout_rate:\n",
        "            n_to_change = int(abs(current_rate - dropout_rate) * n_samples)\n",
        "            if current_rate < dropout_rate:\n",
        "                idx = np.where(dropout == 0)[0]\n",
        "                change_idx = np.random.choice(idx, n_to_change, replace=False)\n",
        "                dropout[change_idx] = 1\n",
        "            else:\n",
        "                idx = np.where(dropout == 1)[0]\n",
        "                change_idx = np.random.choice(idx, n_to_change, replace=False)\n",
        "                dropout[change_idx] = 0\n",
        "        return pd.DataFrame({\n",
        "            'student_id': student_ids,\n",
        "            'gpa': gpa,\n",
        "            'attendance': attendance,\n",
        "            'semester': semester,\n",
        "            'prev_gpa': prev_gpa,\n",
        "            'failed_courses': failed_courses,\n",
        "            'feedback_engagement': feedback_engagement,\n",
        "            'late_assignments': late_assignments,\n",
        "            'forum_participation': forum_participation,\n",
        "            'meeting_attendance': meeting_attendance,\n",
        "            'study_group': study_group,\n",
        "            'days_active': days_active,\n",
        "            'clicks_per_week': clicks_per_week,\n",
        "            'assessments_submitted': assessments_submitted,\n",
        "            'previous_attempts': previous_attempts,\n",
        "            'studied_credits': studied_credits,\n",
        "            'dropout': dropout\n",
        "        })\n",
        "    df_raw = generate_student_data(1000, 0.3)\n",
        "\n",
        "# Keep only columns used downstream\n",
        "cols = [c for c in df_raw.columns if c != 'student_id']\n",
        "df = df_raw[cols].copy()\n",
        "\n",
        "# Split\n",
        "X = df.drop(columns=['dropout'])\n",
        "y = df['dropout']\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
        "\n",
        "print(X_train.shape, X_val.shape, X_test.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Isolation Forest and compute anomaly features\n",
        "ANOMALY_FEATURES = ['clicks_per_week', 'days_active', 'previous_attempts', 'studied_credits', 'assessments_submitted']\n",
        "ANOMALY_FEATURES = [f for f in ANOMALY_FEATURES if f in X_train.columns]\n",
        "\n",
        "iso = IsolationForest(n_estimators=100, contamination=0.1, random_state=42, n_jobs=-1)\n",
        "iso.fit(X_train[ANOMALY_FEATURES])\n",
        "\n",
        "def add_anomaly_features(X, model, features):\n",
        "    raw = -model.decision_function(X[features])\n",
        "    scores = (raw - raw.min()) / (raw.max() - raw.min() + 1e-12)\n",
        "    flags = (model.predict(X[features]) == -1).astype(int)\n",
        "    X_enh = X.copy()\n",
        "    X_enh['anomaly_score'] = scores\n",
        "    X_enh['is_anomaly'] = flags\n",
        "    if 'gpa' in X_enh.columns:\n",
        "        X_enh['anomaly_gpa_interaction'] = X_enh['anomaly_score'] * X_enh['gpa']\n",
        "    if 'attendance' in X_enh.columns:\n",
        "        X_enh['anomaly_attendance_interaction'] = X_enh['anomaly_score'] * X_enh['attendance']\n",
        "    return X_enh, scores, flags\n",
        "\n",
        "X_train_enh, train_anom_scores, train_is_anom = add_anomaly_features(X_train, iso, ANOMALY_FEATURES)\n",
        "X_val_enh, val_anom_scores, val_is_anom = add_anomaly_features(X_val, iso, ANOMALY_FEATURES)\n",
        "X_test_enh, test_anom_scores, test_is_anom = add_anomaly_features(X_test, iso, ANOMALY_FEATURES)\n",
        "\n",
        "print(X_train_enh.shape, X_val_enh.shape, X_test_enh.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Random Forest with SMOTE and find F1-optimal threshold\n",
        "smote = SMOTE(random_state=42, sampling_strategy=0.6)\n",
        "X_res, y_res = smote.fit_resample(X_train_enh, y_train)\n",
        "\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=10,\n",
        "    min_samples_split=5,\n",
        "    min_samples_leaf=3,\n",
        "    max_features='sqrt',\n",
        "    bootstrap=True,\n",
        "    class_weight={0: 1, 1: 10},\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf.fit(X_res, y_res)\n",
        "\n",
        "# Validation probabilities and threshold\n",
        "val_probs = rf.predict_proba(X_val_enh)[:, 1]\n",
        "prec, rec, thr = precision_recall_curve(y_val, val_probs)\n",
        "f1s = 2 * (prec * rec) / (prec + rec + 1e-12)\n",
        "opt_idx = np.nanargmax(f1s[:-1])  # last precision point has no threshold\n",
        "opt_thr = thr[opt_idx]\n",
        "print('Optimal threshold (F1 on val):', round(float(opt_thr), 3))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RF: Curves, confusion matrix, metrics table\n",
        "from sklearn.metrics import PrecisionRecallDisplay, RocCurveDisplay\n",
        "\n",
        "def rf_reports(model, Xv, yv, thr, name='Validation'):\n",
        "    probs = model.predict_proba(Xv)[:, 1]\n",
        "    preds = (probs >= thr).astype(int)\n",
        "    cm = confusion_matrix(yv, preds)\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    metrics = {\n",
        "        'dataset': name,\n",
        "        'threshold': float(thr),\n",
        "        'accuracy': accuracy_score(yv, preds),\n",
        "        'precision': precision_score(yv, preds, zero_division=0),\n",
        "        'recall': recall_score(yv, preds, zero_division=0),\n",
        "        'f1': f1_score(yv, preds, zero_division=0),\n",
        "        'specificity': tn / (tn + fp + 1e-12),\n",
        "        'roc_auc': roc_auc_score(yv, probs)\n",
        "    }\n",
        "    # Save metrics\n",
        "    mdf = pd.DataFrame([metrics])\n",
        "    mdf.to_csv(f\"{TAB_DIR}/rf_metrics_{name.lower()}.csv\", index=False)\n",
        "\n",
        "    # Confusion matrix plot\n",
        "    plt.figure(figsize=(5,4))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['No Dropout','Dropout'],\n",
        "                yticklabels=['No Dropout','Dropout'])\n",
        "    plt.title(f'RF Confusion Matrix - {name} (thr={thr:.2f})')\n",
        "    plt.xlabel('Predicted'); plt.ylabel('Actual')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{FIG_DIR}/rf_confmat_{name.lower()}.png\", dpi=200)\n",
        "    plt.show()\n",
        "\n",
        "    # ROC curve\n",
        "    RocCurveDisplay.from_predictions(yv, probs)\n",
        "    plt.title(f'RF ROC - {name} (AUC={metrics[\"roc_auc\"]:.2f})')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{FIG_DIR}/rf_roc_{name.lower()}.png\", dpi=200)\n",
        "    plt.show()\n",
        "\n",
        "    # PR curve with F1-optimal marker\n",
        "    precision, recall, thresholds = precision_recall_curve(yv, probs)\n",
        "    f1s = 2 * (precision * recall) / (precision + recall + 1e-12)\n",
        "    best_idx = np.nanargmax(f1s[:-1])\n",
        "\n",
        "    plt.figure(figsize=(5,4))\n",
        "    plt.plot(recall, precision, label='PR')\n",
        "    plt.scatter(recall[best_idx], precision[best_idx], c='red', label='F1-optimal')\n",
        "    plt.xlabel('Recall'); plt.ylabel('Precision')\n",
        "    plt.title(f'RF PR Curve - {name}')\n",
        "    plt.legend(); plt.tight_layout()\n",
        "    plt.savefig(f\"{FIG_DIR}/rf_pr_{name.lower()}.png\", dpi=200)\n",
        "    plt.show()\n",
        "\n",
        "    return metrics\n",
        "\n",
        "val_metrics = rf_reports(rf, X_val_enh, y_val, opt_thr, name='Validation')\n",
        "test_metrics = rf_reports(rf, X_test_enh, y_test, opt_thr, name='Test')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dempster–Shafer: combiner, results, and threshold sweep\n",
        "class DempsterShaferCombination:\n",
        "    def __init__(self, classes=(\"non-dropout\",\"dropout\")):\n",
        "        self.classes = classes\n",
        "        self.frame = [set(), {classes[0]}, {classes[1]}, set(classes)]\n",
        "\n",
        "    def _convert_proba_to_mass(self, proba, uncertainty=0.2):\n",
        "        p = float(np.clip(proba, 1e-4, 1-1e-4))\n",
        "        m = {\n",
        "            frozenset(): 0.0,\n",
        "            frozenset({self.classes[0]}): (1 - p) * (1 - uncertainty),\n",
        "            frozenset({self.classes[1]}): p * (1 - uncertainty),\n",
        "            frozenset(self.classes): uncertainty\n",
        "        }\n",
        "        return m\n",
        "\n",
        "    def _combine_masses(self, m1, m2):\n",
        "        out = {}\n",
        "        k = 0.0\n",
        "        for a, va in m1.items():\n",
        "            for b, vb in m2.items():\n",
        "                inter = frozenset(set(a).intersection(set(b)))\n",
        "                prod = va * vb\n",
        "                if len(inter) == 0:\n",
        "                    k += prod\n",
        "                else:\n",
        "                    out[inter] = out.get(inter, 0.0) + prod\n",
        "        if k < 1.0:\n",
        "            for key in out:\n",
        "                out[key] /= (1 - k)\n",
        "        for s in [frozenset(), frozenset({self.classes[0]}), frozenset({self.classes[1]}), frozenset(self.classes)]:\n",
        "            out.setdefault(s, 0.0)\n",
        "        return out\n",
        "\n",
        "    def combine(self, anomaly_score, clf_proba, expert_score=None):\n",
        "        m_anom = self._convert_proba_to_mass(anomaly_score, uncertainty=0.25)\n",
        "        m_clf  = self._convert_proba_to_mass(clf_proba,       uncertainty=0.15)\n",
        "        m = self._combine_masses(m_anom, m_clf)\n",
        "        if expert_score is not None:\n",
        "            m_exp = self._convert_proba_to_mass(expert_score, uncertainty=0.20)\n",
        "            m = self._combine_masses(m, m_exp)\n",
        "        drop = frozenset({self.classes[1]})\n",
        "        belief = m[drop]\n",
        "        plaus = belief + m[frozenset(self.classes)]\n",
        "        return belief, plaus, plaus - belief\n",
        "\n",
        "# Expert heuristic (optional)\n",
        "def expert_rule(df):\n",
        "    g = (df['gpa'] < 2.0).astype(float) * 0.5 if 'gpa' in df else 0.0\n",
        "    a = (df['attendance'] < 65).astype(float) * 0.3 if 'attendance' in df else 0.0\n",
        "    f = (df['failed_courses'] > 3).astype(float) * 0.2 if 'failed_courses' in df else 0.0\n",
        "    if isinstance(g, (int, float)):\n",
        "        return np.zeros(len(df)) + (g + a + f)\n",
        "    return g + a + f\n",
        "\n",
        "# Build DS results on test set\n",
        "rf_test_probs = rf.predict_proba(X_test_enh)[:, 1]\n",
        "ds = DempsterShaferCombination()\n",
        "exp = expert_rule(X_test_enh)\n",
        "\n",
        "belief, plaus, uncert = [], [], []\n",
        "for i in range(len(X_test_enh)):\n",
        "    b, p, u = ds.combine(test_anom_scores[i], rf_test_probs[i], exp[i])\n",
        "    belief.append(b); plaus.append(p); uncert.append(u)\n",
        "\n",
        "belief = np.array(belief); plaus = np.array(plaus); uncert = np.array(uncert)\n",
        "\n",
        "# Belief–plausibility scatter\n",
        "plt.figure(figsize=(6,5))\n",
        "sc = plt.scatter(belief, plaus, c=uncert, cmap='YlOrRd', alpha=0.7)\n",
        "plt.colorbar(sc, label='Uncertainty')\n",
        "plt.plot([0,1],[0,1],'k--',alpha=0.4)\n",
        "plt.axvline(0.5, ls=':', c='gray'); plt.axhline(0.5, ls=':', c='gray')\n",
        "plt.xlabel('Belief (Dropout)'); plt.ylabel('Plausibility (Dropout)')\n",
        "plt.title('Belief–Plausibility (Test)')\n",
        "plt.tight_layout(); plt.savefig(f\"{FIG_DIR}/ds_belief_plaus_test.png\", dpi=200); plt.show()\n",
        "\n",
        "# Risk tiers by belief\n",
        "risk = pd.cut(belief, bins=[-1,0.3,0.5,0.7,1.0], labels=['Low Risk','Moderate Risk','High Risk','Very High Risk'])\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.countplot(x=risk, hue=y_test, palette='Set2')\n",
        "plt.title('Risk Tiers vs True Labels (Test)')\n",
        "plt.xlabel('Risk Tier'); plt.ylabel('Count')\n",
        "plt.tight_layout(); plt.savefig(f\"{FIG_DIR}/ds_tiers_vs_labels_test.png\", dpi=200); plt.show()\n",
        "\n",
        "# Interval coverage\n",
        "covered = ((y_test.values==1) & (plaus>0.5)) | ((y_test.values==0) & (belief<0.5))\n",
        "coverage = covered.mean(); avg_unc = float(uncert.mean())\n",
        "pd.DataFrame([{'dataset':'Test','interval_coverage':coverage,'avg_uncertainty':avg_unc}]).to_csv(f\"{TAB_DIR}/ds_interval_coverage_test.csv\", index=False)\n",
        "print('Interval coverage:', round(coverage,3), '| Avg uncertainty:', round(avg_unc,3))\n",
        "\n",
        "# Specificity–Recall vs threshold for belief\n",
        "ths = np.linspace(0.1,0.9,17)\n",
        "rows = []\n",
        "for t in ths:\n",
        "    yhat = (belief >= t).astype(int)\n",
        "    tn, fp, fn, tp = confusion_matrix(y_test, yhat).ravel()\n",
        "    spec = tn/(tn+fp+1e-12); rec = tp/(tp+fn+1e-12)\n",
        "    rows.append({'thr':float(t),'specificity':spec,'recall':rec})\n",
        "cur = pd.DataFrame(rows)\n",
        "cur.to_csv(f\"{TAB_DIR}/ds_spec_recall_sweep_test.csv\", index=False)\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(cur['recall'], cur['specificity'], marker='o')\n",
        "plt.xlabel('Recall'); plt.ylabel('Specificity')\n",
        "plt.title('DS Specificity–Recall Trade-off (Test)')\n",
        "plt.grid(alpha=0.3); plt.tight_layout(); plt.savefig(f\"{FIG_DIR}/ds_spec_recall_tradeoff_test.png\", dpi=200); plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature importance (top 10)\n",
        "import numpy as np\n",
        "\n",
        "importances = rf.feature_importances_\n",
        "idx = np.argsort(importances)[::-1][:10]\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.bar(range(len(idx)), importances[idx])\n",
        "plt.xticks(range(len(idx)), [X_train_enh.columns[i] for i in idx], rotation=45, ha='right')\n",
        "plt.title('Random Forest Feature Importance (Top 10)')\n",
        "plt.tight_layout(); plt.savefig(f\"{FIG_DIR}/rf_feature_importance_top10.png\", dpi=200); plt.show()\n",
        "\n",
        "fi_table = pd.DataFrame({'feature': X_train_enh.columns, 'importance': importances}).sort_values('importance', ascending=False)\n",
        "fi_table.to_csv(f\"{TAB_DIR}/rf_feature_importance.csv\", index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ablation: RF w/o anomaly-derived features vs with\n",
        "BASE_FEATURES = [c for c in X_train.columns]\n",
        "\n",
        "# Train baseline RF (no anomaly features)\n",
        "X_res_b, y_res_b = smote.fit_resample(X_train[BASE_FEATURES], y_train)\n",
        "rf_base = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1, class_weight={0:1,1:10})\n",
        "rf_base.fit(X_res_b, y_res_b)\n",
        "\n",
        "# Evaluate on validation\n",
        "val_probs_b = rf_base.predict_proba(X_val[BASE_FEATURES])[:,1]\n",
        "prec_b, rec_b, thr_b = precision_recall_curve(y_val, val_probs_b)\n",
        "f1s_b = 2*(prec_b*rec_b)/(prec_b+rec_b+1e-12)\n",
        "opt_b = thr_b[np.nanargmax(f1s_b[:-1])]\n",
        "\n",
        "# Build a comparison table\n",
        "def eval_at(model, Xv, yv, thr):\n",
        "    p = model.predict_proba(Xv)[:,1]\n",
        "    yhat = (p>=thr).astype(int)\n",
        "    tn, fp, fn, tp = confusion_matrix(yv, yhat).ravel()\n",
        "    return {\n",
        "        'accuracy': accuracy_score(yv, yhat),\n",
        "        'precision': precision_score(yv, yhat, zero_division=0),\n",
        "        'recall': recall_score(yv, yhat, zero_division=0),\n",
        "        'f1': f1_score(yv, yhat, zero_division=0),\n",
        "        'specificity': tn/(tn+fp+1e-12),\n",
        "        'roc_auc': roc_auc_score(yv, p),\n",
        "        'threshold': float(thr)\n",
        "    }\n",
        "\n",
        "ablation = pd.DataFrame({\n",
        "    'model': ['RF_base','RF_enhanced'],\n",
        "    'dataset': ['Validation','Validation'],\n",
        "    **pd.DataFrame([\n",
        "        eval_at(rf_base, X_val[BASE_FEATURES], y_val, opt_b),\n",
        "        eval_at(rf, X_val_enh, y_val, opt_thr)\n",
        "    ]).to_dict(orient='list')\n",
        "})\n",
        "\n",
        "ablation.to_csv(f\"{TAB_DIR}/ablation_validation.csv\", index=False)\n",
        "print(ablation)\n",
        "\n",
        "# Plot PR curves overlay\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(rec_b, prec_b, label='RF base (no anomaly)')\n",
        "prec_e, rec_e, _ = precision_recall_curve(y_val, rf.predict_proba(X_val_enh)[:,1])\n",
        "plt.plot(rec_e, prec_e, label='RF enhanced (with anomaly)')\n",
        "plt.xlabel('Recall'); plt.ylabel('Precision'); plt.title('PR curves (Validation)')\n",
        "plt.legend(); plt.tight_layout(); plt.savefig(f\"{FIG_DIR}/ablation_pr_validation.png\", dpi=200); plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Decision analysis: cost-sensitive sweep (FN cost vs FP cost)\n",
        "# Define cost = C_fn*FN + C_fp*FP per threshold; plot net benefit-like surface\n",
        "\n",
        "C_fn_values = [1, 2, 5, 10]\n",
        "C_fp = 1.0\n",
        "probs = rf.predict_proba(X_test_enh)[:,1]\n",
        "ths = np.linspace(0.05, 0.95, 19)\n",
        "rows = []\n",
        "for cfn in C_fn_values:\n",
        "    for t in ths:\n",
        "        yhat = (probs >= t).astype(int)\n",
        "        tn, fp, fn, tp = confusion_matrix(y_test, yhat).ravel()\n",
        "        cost = cfn*fn + C_fp*fp\n",
        "        rows.append({'C_fn':cfn,'thr':float(t),'FN':int(fn),'FP':int(fp),'TP':int(tp),'TN':int(tn),'cost':float(cost)})\n",
        "\n",
        "cost_df = pd.DataFrame(rows)\n",
        "cost_df.to_csv(f\"{TAB_DIR}/decision_cost_sweep_test.csv\", index=False)\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "for cfn in C_fn_values:\n",
        "    sub = cost_df[cost_df['C_fn']==cfn]\n",
        "    plt.plot(sub['thr'], sub['cost'], label=f'C_fn={cfn}')\n",
        "plt.xlabel('Threshold'); plt.ylabel('Total Cost (C_fn*FN + C_fp*FP)')\n",
        "plt.title('Decision Cost vs Threshold (Test)')\n",
        "plt.legend(); plt.tight_layout(); plt.savefig(f\"{FIG_DIR}/decision_cost_vs_threshold.png\", dpi=200); plt.show()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
